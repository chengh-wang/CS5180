{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Complete the Implementation of the Four Rooms environment \n",
    "\n",
    "- The FourRooms is implemented as a python class. We explain the attributes and methods as follows\n",
    "    - **init** function: Define all the attributes of the Four Rooms environment. For example, the state space, the action space, the start state, the goal state and so on.\n",
    "    - **reset** function: Resets the agent to the start state (0, 0)\n",
    "    - **step** function: Takes the current state and one action, returns the next state and a reward\n",
    "   \n",
    "- Please complete the implementation in the step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10, 10), None)\n"
     ]
    }
   ],
   "source": [
    "# FOUR ROOM ENVIRONMENT\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # define the four room as a 2-D array for easy state space reference and visualization\n",
    "        # 0 represents an empty cell; 1 represents a wall cell\n",
    "        self.four_room_space = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "        \n",
    "        # find the positions for all empty cells\n",
    "        # not that: the origin for a 2-D numpy array is located at top-left while the origin for the FourRooms is at\n",
    "        # the bottom-left. The following codes performs the re-projection.\n",
    "        empty_cells = np.where(self.four_room_space == 0.0)\n",
    "#         print(empty_cells)\n",
    "        self.state_space = [[col, 10 - row] for row, col in zip(empty_cells[0], empty_cells[1])]\n",
    "\n",
    "        # define the action space\n",
    "        self.action_space = {'LEFT': np.array([-1, 0]),\n",
    "                             'RIGHT': np.array([1, 0]),\n",
    "                             'DOWN': np.array([0, -1]),\n",
    "                             'UP': np.array([0, 1])}\n",
    "        \n",
    "        # define the start state\n",
    "        self.start_state = [0, 0]\n",
    "        \n",
    "        # define the goal state\n",
    "        self.goal_state = [10, 10]\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent's state to the start state [0, 0]\n",
    "        Return both the start state and reward\n",
    "        \"\"\"\n",
    "        state = self.start_state  # reset the agent to [0, 0]\n",
    "        reward = 0  # reward is 0\n",
    "        return state, reward\n",
    "        \n",
    "\n",
    "    def step(self, state, act):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            state: a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
    "            act: a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
    "        Output args: \n",
    "            next_state: a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
    "            reward: an integer. it can be either 0 or 1.\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "        # CODE HERE: implement the stochastic dynamics as described in Q1. \n",
    "        # Please note, we provide you with the deterministic transition function \"take_action\" below.\n",
    "        # Therefore, you only have to implement the logics of the stochasticity.\n",
    "        \n",
    "        if act == \"LEFT\":\n",
    "            act_ = np.random.choice(list(test.action_space.keys()), 1,p = [0.8, 0, 0.1, 0.1])\n",
    "        elif act == \"DOWN\":\n",
    "            act_ = np.random.choice(list(test.action_space.keys()), 1,p = [0.1, 0.1, 0.8, 0])\n",
    "        elif act == \"RIGHT\":\n",
    "            act_ = np.random.choice(list(self.action_space.keys()), 1,p = [0, 0.8, 0.1, 0.1])\n",
    "        elif act == \"UP\":\n",
    "            act_ = np.random.choice(list(self.action_space.keys()), 1,p = [0.1, 0.1, 0, 0.8])\n",
    "\n",
    "        act_ = np.array2string(act_).replace(\"'\", \"-\").split(\"-\")[1]\n",
    "\n",
    "        next_state = np.add(state,self.action_space[act_])\n",
    "\n",
    "        if list(next_state) not in self.state_space:\n",
    "            next_state = state\n",
    "        \n",
    "        # CODE HERE: compute the reward based on the resulting state\n",
    "        if next_state == [10,10]:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = None\n",
    "        \n",
    "\n",
    "        # return the current state, reward\n",
    "        return next_state, reward\n",
    "        \n",
    "\n",
    "    \"\"\" DO NOT CHANGE BELOW \"\"\"\n",
    "    def take_action(self, state, act):\n",
    "        \"\"\"\n",
    "        Input args: \n",
    "            state (list): a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
    "            act (string): a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
    "        Output args: \n",
    "            next_state (list): a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        next_state = state + self.action_space[act]\n",
    "        return next_state.tolist() if next_state.tolist() in self.state_space else state.tolist()\n",
    "    \n",
    "test = FourRooms()\n",
    "print(test.step((10,10),'UP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the plot function you can use to generate the figure. DO NOT CHANGE\"\"\"\n",
    "# PLOT FUNCTION\n",
    "def plot_func(res_list):\n",
    "    # set the figure size\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # plot each trial\n",
    "    for re in res_list:\n",
    "        plt.plot(list(range(len(res_list[0]))), re, linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "\n",
    "    # plot mean reward\n",
    "    mean_reward = np.array(res_list).mean(axis=0).tolist()\n",
    "    plt.plot(list(range(len(res_list[0]))), mean_reward, linestyle=\"-\", linewidth=2, color=\"k\")\n",
    "\n",
    "    # plot the figure\n",
    "    plt.ylabel(\"Cumulative reward\")\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Implement the manual policy\n",
    "\n",
    "Use this to check your whether your implementation of the step function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step = 0, state = [0, 0], action = None, next state = None, reward = None\n",
      "Step = 1, state = None, action = None, next state = None, reward = None\n",
      "Step = 2, state = None, action = None, next state = None, reward = None\n",
      "Step = 3, state = None, action = None, next state = None, reward = None\n",
      "Step = 4, state = None, action = None, next state = None, reward = None\n",
      "Step = 5, state = None, action = None, next state = None, reward = None\n",
      "Step = 6, state = None, action = None, next state = None, reward = None\n",
      "Step = 7, state = None, action = None, next state = None, reward = None\n",
      "Step = 8, state = None, action = None, next state = None, reward = None\n",
      "Step = 9, state = None, action = None, next state = None, reward = None\n",
      "Step = 10, state = None, action = None, next state = None, reward = None\n",
      "Step = 11, state = None, action = None, next state = None, reward = None\n",
      "Step = 12, state = None, action = None, next state = None, reward = None\n",
      "Step = 13, state = None, action = None, next state = None, reward = None\n",
      "Step = 14, state = None, action = None, next state = None, reward = None\n",
      "Step = 15, state = None, action = None, next state = None, reward = None\n",
      "Step = 16, state = None, action = None, next state = None, reward = None\n",
      "Step = 17, state = None, action = None, next state = None, reward = None\n",
      "Step = 18, state = None, action = None, next state = None, reward = None\n",
      "Step = 19, state = None, action = None, next state = None, reward = None\n",
      "Step = 20, state = None, action = None, next state = None, reward = None\n",
      "Step = 21, state = None, action = None, next state = None, reward = None\n",
      "Step = 22, state = None, action = None, next state = None, reward = None\n",
      "Step = 23, state = None, action = None, next state = None, reward = None\n",
      "Step = 24, state = None, action = None, next state = None, reward = None\n",
      "Step = 25, state = None, action = None, next state = None, reward = None\n",
      "Step = 26, state = None, action = None, next state = None, reward = None\n",
      "Step = 27, state = None, action = None, next state = None, reward = None\n",
      "Step = 28, state = None, action = None, next state = None, reward = None\n",
      "Step = 29, state = None, action = None, next state = None, reward = None\n",
      "Step = 30, state = None, action = None, next state = None, reward = None\n",
      "Step = 31, state = None, action = None, next state = None, reward = None\n",
      "Step = 32, state = None, action = None, next state = None, reward = None\n",
      "Step = 33, state = None, action = None, next state = None, reward = None\n",
      "Step = 34, state = None, action = None, next state = None, reward = None\n",
      "Step = 35, state = None, action = None, next state = None, reward = None\n",
      "Step = 36, state = None, action = None, next state = None, reward = None\n",
      "Step = 37, state = None, action = None, next state = None, reward = None\n",
      "Step = 38, state = None, action = None, next state = None, reward = None\n",
      "Step = 39, state = None, action = None, next state = None, reward = None\n",
      "Step = 40, state = None, action = None, next state = None, reward = None\n",
      "Step = 41, state = None, action = None, next state = None, reward = None\n",
      "Step = 42, state = None, action = None, next state = None, reward = None\n",
      "Step = 43, state = None, action = None, next state = None, reward = None\n",
      "Step = 44, state = None, action = None, next state = None, reward = None\n",
      "Step = 45, state = None, action = None, next state = None, reward = None\n",
      "Step = 46, state = None, action = None, next state = None, reward = None\n",
      "Step = 47, state = None, action = None, next state = None, reward = None\n",
      "Step = 48, state = None, action = None, next state = None, reward = None\n",
      "Step = 49, state = None, action = None, next state = None, reward = None\n",
      "Step = 50, state = None, action = None, next state = None, reward = None\n",
      "Step = 51, state = None, action = None, next state = None, reward = None\n",
      "Step = 52, state = None, action = None, next state = None, reward = None\n",
      "Step = 53, state = None, action = None, next state = None, reward = None\n",
      "Step = 54, state = None, action = None, next state = None, reward = None\n",
      "Step = 55, state = None, action = None, next state = None, reward = None\n",
      "Step = 56, state = None, action = None, next state = None, reward = None\n",
      "Step = 57, state = None, action = None, next state = None, reward = None\n",
      "Step = 58, state = None, action = None, next state = None, reward = None\n",
      "Step = 59, state = None, action = None, next state = None, reward = None\n",
      "Step = 60, state = None, action = None, next state = None, reward = None\n",
      "Step = 61, state = None, action = None, next state = None, reward = None\n",
      "Step = 62, state = None, action = None, next state = None, reward = None\n",
      "Step = 63, state = None, action = None, next state = None, reward = None\n",
      "Step = 64, state = None, action = None, next state = None, reward = None\n",
      "Step = 65, state = None, action = None, next state = None, reward = None\n",
      "Step = 66, state = None, action = None, next state = None, reward = None\n",
      "Step = 67, state = None, action = None, next state = None, reward = None\n",
      "Step = 68, state = None, action = None, next state = None, reward = None\n",
      "Step = 69, state = None, action = None, next state = None, reward = None\n",
      "Step = 70, state = None, action = None, next state = None, reward = None\n",
      "Step = 71, state = None, action = None, next state = None, reward = None\n",
      "Step = 72, state = None, action = None, next state = None, reward = None\n",
      "Step = 73, state = None, action = None, next state = None, reward = None\n",
      "Step = 74, state = None, action = None, next state = None, reward = None\n",
      "Step = 75, state = None, action = None, next state = None, reward = None\n",
      "Step = 76, state = None, action = None, next state = None, reward = None\n",
      "Step = 77, state = None, action = None, next state = None, reward = None\n",
      "Step = 78, state = None, action = None, next state = None, reward = None\n",
      "Step = 79, state = None, action = None, next state = None, reward = None\n",
      "Step = 80, state = None, action = None, next state = None, reward = None\n",
      "Step = 81, state = None, action = None, next state = None, reward = None\n",
      "Step = 82, state = None, action = None, next state = None, reward = None\n",
      "Step = 83, state = None, action = None, next state = None, reward = None\n",
      "Step = 84, state = None, action = None, next state = None, reward = None\n",
      "Step = 85, state = None, action = None, next state = None, reward = None\n",
      "Step = 86, state = None, action = None, next state = None, reward = None\n",
      "Step = 87, state = None, action = None, next state = None, reward = None\n",
      "Step = 88, state = None, action = None, next state = None, reward = None\n",
      "Step = 89, state = None, action = None, next state = None, reward = None\n",
      "Step = 90, state = None, action = None, next state = None, reward = None\n",
      "Step = 91, state = None, action = None, next state = None, reward = None\n",
      "Step = 92, state = None, action = None, next state = None, reward = None\n",
      "Step = 93, state = None, action = None, next state = None, reward = None\n",
      "Step = 94, state = None, action = None, next state = None, reward = None\n",
      "Step = 95, state = None, action = None, next state = None, reward = None\n",
      "Step = 96, state = None, action = None, next state = None, reward = None\n",
      "Step = 97, state = None, action = None, next state = None, reward = None\n",
      "Step = 98, state = None, action = None, next state = None, reward = None\n",
      "Step = 99, state = None, action = None, next state = None, reward = None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "    state, reward = env.reset()  # always call reset() before interaction\n",
    "    \n",
    "    # manual time step (YOU CAN CHANGE THIS TO ANY TIME STEP YOU WANT)\n",
    "    time_step = 100\n",
    "\n",
    "    # create a loop\n",
    "    for t in range(time_step):\n",
    "        \n",
    "        # CODE HERE: implement your manual agent/policy function that takes in the action from the standard input\n",
    "        action = None\n",
    "        \n",
    "        # CODE HERE: implement the code to interact with the Four Rooms environment above.\n",
    "        # it should takes in the current state and action and returns the next_state and a reward\n",
    "        # Hint: use the step function that you implement.\n",
    "        next_state, reward = None, None\n",
    "        \n",
    "        \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "        # print interaction\n",
    "        print(f\"Step = {t}, state = {state}, action = {action}, next state = {next_state}, reward = {reward}\")\n",
    "        \n",
    "        # reset if the agent reaches the goal\n",
    "        if reward == 1:\n",
    "            print(\"Reset the agent to the start state!\")\n",
    "            state, reward = env.reset()\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Implement a random policy\n",
    "\n",
    "We provide the scaffolding code for running and plotting. Please implement a random policy\n",
    "\n",
    "**Please note: you should read the code carefully before implementing to make sure the variable names are aligned.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f9276602514446be1e5b0bd260d25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Run trail:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14aed6ed55204fb3985bef686c7f50d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT CHANGE BELOW\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# save the reward\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m reward_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     44\u001b[0m reward_per_trial\u001b[38;5;241m.\u001b[39mappend(reward_counter)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# reset\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "\n",
    "    # number of the trail (YOU CAN MODIFIED HERE WITH SMALL VALUES FOR DEBUG ONLY)\n",
    "    trial_num = 10   \n",
    "    # length of each trail (YOU CAN MODIFIED HERE WITH SMALL VALUES FOR DEBUG ONLY)\n",
    "    trial_length = int(1e4)\n",
    "    \n",
    "    # save the rewards for plot\n",
    "    rewards_list = []\n",
    "    \n",
    "    # run experiment\n",
    "    for e in tqdm.tqdm(range(trial_num), desc=\"Run trail\", position=0):\n",
    "        \n",
    "        # reset for every trail\n",
    "        reward_per_trial = []\n",
    "        reward_counter = 0\n",
    "    \n",
    "        # reset the environment \n",
    "        state, reward = env.reset()\n",
    "        \n",
    "        # run each trial\n",
    "        for t in tqdm.tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
    "            \n",
    "            \n",
    "            # CODE HERE: please implement a random policy to obtain an action.\n",
    "            # it should return a random action from [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "            action = None\n",
    "                       \n",
    "            # CODE HERE: please implement the code to get the next state and reward\n",
    "            # it should takes in the current state and action\n",
    "            # it should returns the next_state and reward\n",
    "            next_state, reward = None, None\n",
    "            \n",
    "            \n",
    "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "            # save the reward\n",
    "            reward_counter += reward\n",
    "            reward_per_trial.append(reward_counter)\n",
    "            \n",
    "            # reset\n",
    "            if reward == 1:\n",
    "                state, reward = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        # save the rewards\n",
    "        rewards_list.append(reward_per_trial)\n",
    "        \n",
    "# PLOT THE RESULTS\n",
    "plot_func(rewards_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Implement better & worse policies against the Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "\n",
    "    # number of the trail\n",
    "    trial_num = 10   \n",
    "    # length of each trail\n",
    "    trial_length = int(1e4)\n",
    "    \n",
    "    # save the rewards for plot\n",
    "    rewards_list = []\n",
    "    \n",
    "    # run experiment\n",
    "    for e in tqdm.tqdm(range(trial_num), desc=\"Run trail\", position=0):\n",
    "        \n",
    "        # reset for every trail\n",
    "        reward_per_trial = []\n",
    "        reward_counter = 0\n",
    "    \n",
    "        # reset the environment \n",
    "        state, reward = env.reset()\n",
    "        \n",
    "        # run each trial\n",
    "        for t in tqdm.tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
    "            \n",
    "            \n",
    "            # CODE HERE: please implement a policy that is worse than the random policy.\n",
    "            # It should takes in the current state and output an action\n",
    "            action = None\n",
    "                       \n",
    "            # CODE HERE: please implement the code to get the next state and reward\n",
    "            next_state, reward = None, None\n",
    "            \n",
    "            \n",
    "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "            # save the reward\n",
    "            reward_counter += reward\n",
    "            reward_per_trial.append(reward_counter)\n",
    "            \n",
    "            # reset\n",
    "            if reward == 1:\n",
    "                state, reward = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        # save the rewards\n",
    "        rewards_list.append(reward_per_trial)\n",
    "        \n",
    "# PLOT THE RESULTS\n",
    "plot_func(rewards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "\n",
    "    # number of the trail\n",
    "    trial_num = 10   \n",
    "    # length of each trail\n",
    "    trial_length = int(1e4)\n",
    "    \n",
    "    # save the rewards for plot\n",
    "    rewards_list = []\n",
    "    \n",
    "    # run experiment\n",
    "    for e in tqdm.tqdm(range(trial_num), desc=\"Run trail\", position=0):\n",
    "        \n",
    "        # reset for every trail\n",
    "        reward_per_trial = []\n",
    "        reward_counter = 0\n",
    "    \n",
    "        # reset the environment \n",
    "        state, reward = env.reset()\n",
    "        \n",
    "        # run each trial\n",
    "        for t in tqdm.tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
    "            \n",
    "            \n",
    "            # CODE HERE: please implement a policy that is better than the random policy.\n",
    "            # It should takes in the current state and output an action\n",
    "            action = None\n",
    "                       \n",
    "            # CODE HERE: please implement the code to get the next state and reward\n",
    "            next_state, reward = None, None\n",
    "            \n",
    "            \n",
    "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "            # save the reward\n",
    "            reward_counter += reward\n",
    "            reward_per_trial.append(reward_counter)\n",
    "            \n",
    "            # reset\n",
    "            if reward == 1:\n",
    "                state, reward = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        # save the rewards\n",
    "        rewards_list.append(reward_per_trial)\n",
    "        \n",
    "# PLOT THE RESULTS\n",
    "plot_func(rewards_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
